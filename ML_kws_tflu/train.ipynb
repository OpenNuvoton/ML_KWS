{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d35f286",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Section\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf9265f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011d390d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Location:\n",
      "/ML_KWS/ML_kws_tflu\n"
     ]
    }
   ],
   "source": [
    "#folder_exc = r'C:\\Users\\USERNAME\\MICRO_ML\\ML-examples-main\\tflu-kws-cortex-m\\Training'\n",
    "folder_exc = r'C:\\Users\\garyc\\ML_kws_tflu-main'\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    #drive.mount('/content/drive')\n",
    "    print('Colab in:')\n",
    "    print (os.getcwd())\n",
    "    \n",
    "except ImportError:\n",
    "    print(r'Running Location:')\n",
    "    print(os.path.abspath(os.getcwd()))\n",
    "    #if (os.getcwd() != folder_exc)&(os.getcwd() != folder_exc.replace('/', \"\\\\\")):  \n",
    "    #  os.chdir(folder_exc)\n",
    "    #  print('update to:')\n",
    "    #  print (os.getcwd())\n",
    "    #else:\n",
    "    #  print('no update')  \n",
    "from kws_python import data\n",
    "from kws_python import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7f406",
   "metadata": {
    "id": "fcc7f406",
    "tags": []
   },
   "source": [
    "# Training Section\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4597593f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(FLAGS, save_cmd_fileName):\n",
    "    \n",
    "    #print(FLAGS.data_exist, FLAGS.model_size_info)\n",
    "    \n",
    "    model_settings = models.prepare_model_settings(len(data.prepare_words_list(FLAGS.wanted_words.split(','))),\n",
    "                                                   FLAGS.sample_rate, FLAGS.clip_duration_ms, FLAGS.window_size_ms,\n",
    "                                                   FLAGS.window_stride_ms, FLAGS.dct_coefficient_count)\n",
    "\n",
    "    # Create the model.\n",
    "    model = models.create_model(model_settings, FLAGS.model_architecture, FLAGS.model_size_info, True)\n",
    "\n",
    "    audio_processor = data.AudioProcessor(data_exist=FLAGS.data_exist,\n",
    "                                          data_url=FLAGS.data_url,\n",
    "                                          data_dir=FLAGS.data_dir,\n",
    "                                          silence_percentage=FLAGS.silence_percentage,\n",
    "                                          unknown_percentage=FLAGS.unknown_percentage,\n",
    "                                          wanted_words=FLAGS.wanted_words.split(','),\n",
    "                                          validation_percentage=FLAGS.validation_percentage,\n",
    "                                          testing_percentage=FLAGS.testing_percentage,\n",
    "                                          model_settings=model_settings)\n",
    "\n",
    "      # We decay learning rate in a constant piecewise way to help learning.\n",
    "    training_steps_list = list(map(int, FLAGS.how_many_training_steps.split(',')))\n",
    "    learning_rates_list = list(map(float, FLAGS.learning_rate.split(',')))\n",
    "    lr_boundary_list = training_steps_list[:-1]  # Only need the values at which to change lr.\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=lr_boundary_list,\n",
    "                                                                       values=learning_rates_list)\n",
    "  \n",
    "    # Specify the optimizer configurations.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "    train_data = audio_processor.get_data(audio_processor.Modes.TRAINING,\n",
    "                                          FLAGS.background_frequency, FLAGS.background_volume,\n",
    "                                          int((FLAGS.time_shift_ms * FLAGS.sample_rate) / 1000))\n",
    "    train_data = train_data.repeat().batch(FLAGS.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_data = audio_processor.get_data(audio_processor.Modes.VALIDATION)\n",
    "    val_data = val_data.batch(FLAGS.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "  \n",
    "    # We train for a max number of iterations so need to calculate how many 'epochs' this will be.\n",
    "    training_steps_max = np.sum(training_steps_list)\n",
    "    training_epoch_max = int(np.ceil(training_steps_max / FLAGS.eval_step_interval))\n",
    "    \n",
    "    # Callbacks.\n",
    "    train_dir = Path(FLAGS.train_dir) / \"best\"\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=(train_dir / (FLAGS.model_architecture + \"_{val_accuracy:.3f}_ckpt\")),\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=FLAGS.summaries_dir)\n",
    "    \n",
    "    #Save the train model seeting\n",
    "    src = Path(os.getcwd()) / save_cmd_fileName\n",
    "    dst = Path(os.getcwd()) / Path(FLAGS.train_dir) / save_cmd_fileName\n",
    "    shutil.copy(src, dst)\n",
    "    \n",
    "    # Train the model.\n",
    "    model.fit(x=train_data,\n",
    "              steps_per_epoch=FLAGS.eval_step_interval,\n",
    "              epochs=training_epoch_max,\n",
    "              validation_data=val_data,\n",
    "              callbacks=[model_checkpoint_callback, tensorboard_callback])\n",
    "    \n",
    "    # Test and save the model.\n",
    "    test_data = audio_processor.get_data(audio_processor.Modes.TESTING)\n",
    "    test_data = test_data.batch(FLAGS.batch_size)\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(x=test_data)\n",
    "    print(f'Final test accuracy: {test_acc*100:.2f}%')\n",
    "    \n",
    "    # save result record\n",
    "    forward_pass = tf.function(\n",
    "        model.call,\n",
    "        input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\n",
    "    graph_info = profile(forward_pass.get_concrete_function().graph,\n",
    "                            options=ProfileOptionBuilder.float_operation())\n",
    "    # The //2 is necessary since `profile` counts multiply and accumulate\n",
    "    # as two flops, here we report the total number of multiply accumulate ops\n",
    "    flops = graph_info.total_float_ops // 2\n",
    "    total_para = model.count_params()\n",
    "    print('TensorFlow:', tf.__version__)\n",
    "    print('The MACs of this model: {:,}'.format(flops))\n",
    "    print('The total parameters of this model: {:,}'.format(total_para))\n",
    "    \n",
    "    test_txt_path = os.path.join(os.getcwd(), FLAGS.train_dir, 'result_record.txt')\n",
    "    with open(test_txt_path, 'w') as f:\n",
    "        f.write('Test accuracy: {}'.format(test_acc) + '\\n')\n",
    "        f.write('MACs: {}'.format(flops) + '\\n')\n",
    "        f.write('Total Parameters: {}'.format(total_para) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132a2fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Argument Setting\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8006c4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--data_exist',\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help='True will skip download and tar.')\n",
    "    parser.add_argument(\n",
    "        '--data_url',\n",
    "        type=str,\n",
    "        default='http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n",
    "        help='Location of speech training data archive on the web.')\n",
    "    parser.add_argument(\n",
    "        '--data_dir',\n",
    "        type=str,\n",
    "        default='tmp/speech_dataset',\n",
    "        help=\"\"\"\\\n",
    "        Where to download the speech training data to.\n",
    "        \"\"\")\n",
    "    parser.add_argument(\n",
    "        '--background_volume',\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"\"\"\\\n",
    "        How loud the background noise should be, between 0 and 1.\n",
    "        \"\"\")\n",
    "    parser.add_argument(\n",
    "        '--background_frequency',\n",
    "        type=float,\n",
    "        default=0.8,\n",
    "        help=\"\"\"\\\n",
    "        How many of the training samples have background noise mixed in.\n",
    "        \"\"\")\n",
    "    parser.add_argument(\n",
    "        '--silence_percentage',\n",
    "        type=float,\n",
    "        default=10.0,\n",
    "        help=\"\"\"\\\n",
    "        How much of the training data should be silence.\n",
    "        \"\"\")\n",
    "    parser.add_argument(\n",
    "        '--unknown_percentage',\n",
    "        type=float,\n",
    "        default=10.0,\n",
    "        help=\"\"\"\\\n",
    "        How much of the training data should be unknown words.\n",
    "        \"\"\")\n",
    "    parser.add_argument(\n",
    "        '--time_shift_ms',\n",
    "        type=float,\n",
    "        default=100.0,\n",
    "        help=\"\"\"\\\n",
    "        Range to randomly shift the training audio by in time.\n",
    "        \"\"\")\n",
    "    parser.add_argument(\n",
    "        '--testing_percentage',\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help='What percentage of wavs to use as a test set.')\n",
    "    parser.add_argument(\n",
    "        '--validation_percentage',\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help='What percentage of wavs to use as a validation set.')\n",
    "    parser.add_argument(\n",
    "        '--sample_rate',\n",
    "        type=int,\n",
    "        default=16000,\n",
    "        help='Expected sample rate of the wavs',)\n",
    "    parser.add_argument(\n",
    "        '--clip_duration_ms',\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help='Expected duration in milliseconds of the wavs',)\n",
    "    parser.add_argument(\n",
    "        '--window_size_ms',\n",
    "        type=float,\n",
    "        default=30.0,\n",
    "        help='How long each spectrogram timeslice is',)\n",
    "    parser.add_argument(\n",
    "        '--window_stride_ms',\n",
    "        type=float,\n",
    "        default=10.0,\n",
    "        help='How long each spectrogram timeslice is',)\n",
    "    parser.add_argument(\n",
    "        '--dct_coefficient_count',\n",
    "        type=int,\n",
    "        default=40,\n",
    "        help='How many bins to use for the MFCC fingerprint',)\n",
    "    parser.add_argument(\n",
    "        '--how_many_training_steps',\n",
    "        type=str,\n",
    "        #default='15,3',\n",
    "        default='15000,3000',\n",
    "        help='How many training loops to run',)\n",
    "    parser.add_argument(\n",
    "        '--eval_step_interval',\n",
    "        type=int,\n",
    "        default=400,\n",
    "        help='How often to evaluate the training results.')\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        type=str,\n",
    "        default='0.001,0.0001',\n",
    "        help='How large a learning rate to use when training.')\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help='How many items to train with at once',)\n",
    "    parser.add_argument(\n",
    "        '--summaries_dir',\n",
    "        type=str,\n",
    "        default='/tmp/retrain_logs',\n",
    "        help='Where to save summary logs for TensorBoard.')\n",
    "    parser.add_argument(\n",
    "        '--wanted_words',\n",
    "        type=str,\n",
    "        default='yes,no',\n",
    "        help='Words to use (others will be added to an unknown label)',)\n",
    "    parser.add_argument(\n",
    "        '--train_dir',\n",
    "        type=str,\n",
    "        default='/tmp/speech_commands_train',\n",
    "        help='Directory to write event logs and checkpoint.')\n",
    "    parser.add_argument(\n",
    "        '--model_architecture',\n",
    "        type=str,\n",
    "        default='dnn',\n",
    "        help='What model architecture to use')\n",
    "    parser.add_argument(\n",
    "        '--model_size_info',\n",
    "        type=int,\n",
    "        nargs=\"+\",\n",
    "        default=[128, 128, 128],\n",
    "        help='Model dimensions - different for various models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2b7b0-a778-43e4-a0fa-3c63831b1c59",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Widgets Control Section\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9608889-9419-4f76-89a8-b142f795d3fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive\n",
    "from ipywidgets import AppLayout, Button, Layout, Box, FloatText, Textarea, Dropdown, Label, IntSlider\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Image, clear_output\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class init_train_widgets():\n",
    "    def __init__(self):\n",
    "        form_item_layout = Layout(\n",
    "        display='flex',\n",
    "        flex_flow='row',\n",
    "        justify_content='space-between',\n",
    "        )\n",
    "        \n",
    "        #button_layout = Layout(flex='1 1 auto',\n",
    "        #                      width='auto')\n",
    "        #button_words = ['Start Train']\n",
    "        #self.button_items = [Button(description=w, layout=button_layout, button_style='success') for w in button_words]\n",
    "\n",
    "        ### train model parameters widgets ###\n",
    "        self.A_ta = Dropdown(options=['ds_cnn', 'dnn', 'cnn', 'basic_lstm'])\n",
    "        self.B_ta = widgets.BoundedIntText(value=10, min=0, max=50.0, step=1, disabled=False)\n",
    "        self.C_ta = widgets.BoundedIntText(value=10, min=0, max=50.0, step=1, disabled=False)\n",
    "        self.D_ta = widgets.Text(value='5000,10000,10000', placeholder='Type something', description='String:', disabled=False)\n",
    "        self.E_ta = widgets.Text(value='0.0005,0.0001,0.00002', placeholder='Type something', description='String:', disabled=False)\n",
    "        self.F_ta = widgets.IntSlider(value=400, min=100, max=1500, step=100)\n",
    "        self.G_ta = widgets.IntSlider(value=100, min=50, max=1000, step=50)\n",
    "        self.H_ta = widgets.Text(value='5 64 10 4 2 2 64 3 3 1 1 64 3 3 1 1 64 3 3 1 1 64 3 3 1 1', placeholder='Type something', description='Int:', disabled=False)\n",
    "        self.I_ta = widgets.Textarea(value='yes,no,up,down,left,right,on,off,stop,go', placeholder='Type something', description='String:', disabled=False)\n",
    "        self.J_ta = widgets.Text(value='work/DS_CNN/1/retrain_logs', placeholder='Type something', description='String:', disabled=False)\n",
    "        self.K_ta = widgets.Text(value='work/DS_CNN/1/training', placeholder='Type something', description='String:', disabled=False)\n",
    "        \n",
    " \n",
    "        form_train_items = [\n",
    "            Box([Label(value = 'Model Architecture'), self.A_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Testing percentage'), self.B_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Validation percentage'), self.C_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Training Steps'), self.D_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Learning rates'), self.E_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Eval step interval'), self.F_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Batch size'), self.G_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Model size (dimension)'), self.H_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Wanted words'), self.I_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Summaries directory'), self.J_ta], layout=form_item_layout),\n",
    "            Box([Label(value = 'Train directory'), self.K_ta], layout=form_item_layout)\n",
    "        ]\n",
    "        \n",
    "        self.form_box_train_para = Box(form_train_items, layout=Layout(\n",
    "            display='flex',\n",
    "            flex_flow='column',\n",
    "            border='solid 3px lightblue',\n",
    "            align_items='stretch',\n",
    "            width='50%',\n",
    "        ))\n",
    "        \n",
    "        \n",
    "        ### data parameters widgets ###\n",
    "        self.A_da = IntSlider(value=10, min=10, max=50)\n",
    "        self.B_da = widgets.Checkbox(value=True, disabled=False, indent=False)\n",
    "        self.C_da = widgets.FloatSlider(value=0.5, min=0.0, max=1.0)\n",
    "        self.D_da = widgets.FloatSlider(value=0.9, min=0.0, max=1.0)\n",
    "        self.E_da = widgets.FloatSlider(value=20.0, min=0.0, max=50.0)\n",
    "        self.F_da = widgets.FloatSlider(value=20.0, min=0.0, max=50.0)\n",
    "        self.G_da = widgets.FloatSlider(value=200.0, min=50.0, max=500.0, step=10.0)\n",
    "        self.H_da = widgets.IntSlider(value=16000, min=16000, max=32000, step=16000)\n",
    "        self.I_da = widgets.IntSlider(value=1000, min=800, max=3000, step=200)\n",
    "        self.J_da = widgets.IntSlider(value=40, min=10, max=100, step=10)\n",
    "        self.K_da = widgets.IntSlider(value=20, min=10, max=100, step=10)\n",
    "        \n",
    "        \n",
    "        form_data_items = [\n",
    "            Box([Label(value = 'DCT coefficient count'), self.A_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Data exist'), self.B_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Background volume'), self.C_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Background frequency'), self.D_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Silence percentage'), self.E_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Unknown percentage'), self.F_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Time shift (ms)'), self.G_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Sample rate'), self.H_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Clip duration (ms)'), self.I_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Window size (ms)'), self.J_da], layout=form_item_layout),\n",
    "            Box([Label(value = 'Window stride (ms)'), self.K_da], layout=form_item_layout)\n",
    "        ]\n",
    "        \n",
    "        self.form_box_data_para = Box(form_data_items, layout=Layout(\n",
    "            display='flex',\n",
    "            flex_flow='column',\n",
    "            border='solid 3px lightblue',\n",
    "            align_items='stretch',\n",
    "            width='50%',\n",
    "        ))\n",
    "    \n",
    "    def folder_num_check(self, train_loc, dataset_list_check):\n",
    "        for fld_name in dataset_list_check:\n",
    "            check_fld = os.path.join(train_loc, fld_name)\n",
    "            length = len([entry for entry in os.listdir(check_fld) if os.path.isfile(os.path.join(check_fld, entry))])\n",
    "            if 15 > length:  # need < 15\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def create_command(self, cm_list):\n",
    "        #print(cm_list)\n",
    "        argument_list = ['--model_architecture', '--testing_percentage', '--validation_percentage', '--how_many_training_steps',\n",
    "                         '--learning_rate', '--eval_step_interval', '-batch_size', '--model_size_info', '--wanted_words',\n",
    "                         '--summaries_dir', '--train_dir', \n",
    "                         '--dct_coefficient_count', '--data_exist', '--background_volume','--background_frequency', '--silence_percentage', \n",
    "                         '--unknown_percentage', '--time_shift_ms', '--sample_rate','--clip_duration_ms', '--window_size_ms', '--window_stride_ms']\n",
    "        cm_dict = OrderedDict()\n",
    "    \n",
    "        for idx, val in enumerate(cm_list):\n",
    "            if argument_list[idx] == '--model_size_info':  #transfer from single string to list format\n",
    "                cm_dict[argument_list[idx]] = val.split(',')\n",
    "            else:\n",
    "                cm_dict[argument_list[idx]] = val  \n",
    "        print(cm_dict)        \n",
    "        \n",
    "        with open('train_cmd.txt','w') as f:  #save the complete command for train.py\n",
    "            for key, value in cm_dict.items():\n",
    "                \n",
    "                if(type(value) == list):\n",
    "                    f.write('%s ' % (key))\n",
    "                    for i in range(len(value)):\n",
    "                        f.write('%s ' % (value[i]))\n",
    "                else:    \n",
    "                    f.write('%s %s ' % (key, value))\n",
    "     \n",
    "        return 0\n",
    "        \n",
    "    def show_main(self):   \n",
    "        \n",
    "        intro_text = 'Please Choose the parameters of the training or using the default'\n",
    "        htmlWidget = widgets.HTML(value = f\"<b><font color='lightblue'><font size=4>{intro_text}</b>\")\n",
    "        display(htmlWidget)\n",
    "        \n",
    "        #Create an accordion and put the 2 boxes\n",
    "        accordion = widgets.Accordion(children=[self.form_box_train_para, self.form_box_data_para]).add_class(\"parentstyle\")\n",
    "        #Add a custom style tag to the notebook, you can use dev tool to inspect the class names\n",
    "        display(HTML(\"<style>.parentstyle > .p-Accordion-child > .p-Collapse-header{background-color:green}</style>\"))\n",
    "        accordion.set_title(0, 'Train Setting')\n",
    "        accordion.set_title(1, 'Data Setting')\n",
    "        \n",
    "        \n",
    "        def act_para(model,test_per,vali_per,steps,lr,step_inter,batch,dims,outputs,sum_dir,train_dir,\n",
    "                     dct_coe,data,b_vol,b_freq,silence,unk,t_sft,rate,dura,win_size,win_str):\n",
    "            toggle_train_save = widgets.ToggleButton(description='Save Train Setting', layout=Layout(width='30%', height='30px'), button_style='success')\n",
    "            toggle_run = widgets.ToggleButton(description='Start to Run', layout=Layout(width='30%', height='30px'), button_style='success')\n",
    "            out = widgets.Output(layout=Layout(border = '1px solid green'))\n",
    "            def para_process(obj):\n",
    "                with out:\n",
    "                    if obj['new']:\n",
    "                        self.create_command([model,test_per,vali_per,steps,lr,step_inter,batch,dims,outputs,sum_dir,train_dir,\n",
    "                              dct_coe,data,b_vol,b_freq,silence,unk,t_sft,rate,dura,win_size,win_str])\n",
    "                        \n",
    "                        text0 = 'The training setting is finish and saved'\n",
    "                        html0= widgets.HTML(value = f\"<b><font color='lightblue'><font size=2>{text0}</b>\")\n",
    "                        display(html0)\n",
    "                        \n",
    "                    else:\n",
    "                        print('re-start...')\n",
    "                        #out.clear_output()\n",
    "                        \n",
    "            def run(obj):\n",
    "                with out:\n",
    "                    if obj['new']:\n",
    "                        self.run_test()\n",
    "                    else:\n",
    "                        print('stop')\n",
    "                        #out.clear_output()           \n",
    "                        \n",
    "            toggle_train_save.observe(para_process, 'value')\n",
    "            toggle_run.observe(run, 'value')\n",
    "            display(toggle_train_save, toggle_run)\n",
    "            display(out)\n",
    "                   \n",
    "        \n",
    "        out = widgets.interactive_output(act_para, {'model': self.A_ta, 'test_per': self.B_ta, 'vali_per': self.C_ta, 'steps': self.D_ta,\n",
    "                                                    'lr': self.E_ta, 'step_inter': self.F_ta, 'batch': self.G_ta, 'dims': self.H_ta,\n",
    "                                                    'outputs': self.I_ta, 'sum_dir': self.J_ta, 'train_dir': self.K_ta,\n",
    "                                                    'dct_coe': self.A_da, 'data': self.B_da, 'b_vol': self.C_da, 'b_freq': self.D_da,\n",
    "                                                    'silence': self.E_da, 'unk': self.F_da, 't_sft': self.G_da, 'rate': self.H_da,\n",
    "                                                    'dura': self.I_da, 'win_size': self.J_da,  'win_str': self.K_da})\n",
    "\n",
    "        display(accordion, out)\n",
    "     \n",
    "    \n",
    "    def run_test(self):   ###run the mainprogram\n",
    "        save_cmd_fileName = 'train_cmd.txt'\n",
    "        with open(save_cmd_fileName,'r') as f:  #save the complete command for train.py\n",
    "            train_cmd_line = f.read()\n",
    "        cmd_list = train_cmd_line.split()\n",
    "        print(cmd_list)\n",
    "        \n",
    "        for idx, val in enumerate(cmd_list):\n",
    "            if val == 'False':\n",
    "                print('change to bool')\n",
    "                cmd_list[idx] = False\n",
    "            if val == '--wanted_words':  # get the dataset's name\n",
    "                dataset_list_check = cmd_list[idx+1].split(',')    \n",
    "        \n",
    "        if(cmd_list != []):\n",
    "            print('read the train commands!')\n",
    "        else:\n",
    "            print('The train_cmd.txt is empty!')\n",
    "        \n",
    "        FLAGS, _ = parser.parse_known_args(args = cmd_list)\n",
    "        #FLAGS, _ = parser.parse_known_args(args = ['--model_architecture','dnn','--checkpoint',r'work\\DNN\\DNN3\\training\\best\\dnn_0.835_ckpt',\n",
    "        #'--model_size_info','128','128','128'])\n",
    "        #print(FLAGS)\n",
    "        dataset_loc_for_check = os.path.join(os.getcwd(), 'tmp', 'speech_dataset')\n",
    "        if not self.B_da.value:\n",
    "            train(FLAGS, save_cmd_fileName)\n",
    "            print('Finish')\n",
    "        elif (self.folder_num_check(dataset_loc_for_check, dataset_list_check)):  # if any files < 15, don't run training\n",
    "            print(\"The data is not enough, please > 15 files in each label folder.\")\n",
    "            print(\"The dataset path: {}\".format(dataset_loc_for_check))\n",
    "        else:\n",
    "            train(FLAGS, save_cmd_fileName)\n",
    "            print('Finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea60e3-7da5-49a7-addb-459ae9b6063a",
   "metadata": {},
   "source": [
    "# Run Section\n",
    "---\n",
    "- The detail description of all the parameters is here [meaning](#id-PD)\n",
    "- Please download the google train data at first time ==> click`Data Setting` tab, and unclick the `Data exist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e76ba7b-090f-4409-8c3a-b53af9400689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b295d64b6cda46d1a2ac778f4479e8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<b><font color='lightblue'><font size=4>Please Choose the parameters of the training or using the …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.parentstyle > .p-Accordion-child > .p-Collapse-header{background-color:green}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77548dd393f74a3abc6deeae29852cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Box(children=(Box(children=(Label(value='Model Architecture'), Dropdown(options=('ds_cnn',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101c1b7842e34ce1b8183a0ec1e4c752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "act = init_train_widgets()\n",
    "act.show_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234c522-6760-4bec-be5d-8d2edb26e45a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"id-PD\"></a>\n",
    "# Parameter Description\n",
    "---\n",
    "- This notebook is basing on https://github.com/ARM-software/ML-examples/tree/main/tflu-kws-cortex-m.\n",
    "\n",
    "## Train Setting\n",
    "- `Model Architecture`: What model architecture to use.\n",
    "- `Testing percentage`: What percentage of wavs as a test set.\n",
    "- `Validation percentage`: What percentage of wavs as a validation set.\n",
    "- `Training steps`: How many training loops to run. It matches with the learning rates.\n",
    "- `Learning rates`: How large a learning rate to use when training. It matches with the training steps.\n",
    "- `Eval step interval`: How often to evaluate the training results.\n",
    "- `Batch size`: How many items to train with at once.\n",
    "- `Model size (dimension)`: Model dimensions - different for various models. For more detail, please check the `train_commands.txt`.\n",
    "- `Wanted words`: Words to use (others will be added to an unknown label).\n",
    "- `Summaries directory`: Where to save summary logs for TensorBoard.\n",
    "- `Train directory`: Directory to write event logs and checkpoint(The trained model and weights).\n",
    "\n",
    "## Data Setting\n",
    "- `DCT coefficient count`: How many bins to use for the MFCC fingerprint\n",
    "- `Data exist`: True will skip download and tar the default tensorflow's speech dataset. (Notice)When you first play this notebook, please unclick it for downlowing the train dataset at first time.\n",
    "- `Background volume`: How loud the background noise should be, between 0 and 1.\n",
    "- `Background frequency`: How many of the training samples have background noise mixed in.\n",
    "- `Silence percentage`: How much of the training data should be silence.\n",
    "- `Unknown percentage`: How much of the training data should be unknown words.\n",
    "- `Time shift (ms)`: Range to randomly shift the training audio by in time.\n",
    "- `Sample rate`: Expected sample rate of the wavs.\n",
    "- `Clip duration (ms)`: Expected duration in milliseconds of the wavs.\n",
    "- `Window size (ms)`: How long each spectrogram timeslice is.\n",
    "- `Window stride (ms)`: Window stride in samples for calculating spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89024e38-250c-44a3-9a23-db364332be82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "3ea34302c116e3f1ed1f45b536c02c3c91f48f73d0c8f4e0f4fcbe87898c0385"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
